# jemdoc: menu{MENU}{index.html}, nofooter  
==Siliang Zeng

~~~
{}{img_left}{photo.jpeg}{}{200px}{}
\n [https://cse.umn.edu/ece Electrical and Computer Engineering], \n
[https://twin-cities.umn.edu/ University of Minnesota, Twin Cities]\n
[https://scholar.google.com/citations?user=IfqsDyYAAAAJ&hl=en Google Scholar], [https://www.dropbox.com/scl/fi/hn3vtne77lob53bw8q8eg/Siliang-CV.pdf?rlkey=hrj5hkm3dxehulrdimsygirzv&dl=0 CV] \n
Email: zeng0176 [at] umn (dot) edu
~~~

== About me
I am a fourth year PhD student at [https://twin-cities.umn.edu/ University of Minnesota], advised by [https://people.ece.umn.edu/~mhong/mingyi.html Prof. Mingyi Hong]. \n
Before moving to Minnesota, I received the B.S. in Statistics from [https://www.cuhk.edu.cn/en The Chinese University of Hong Kong, Shenzhen] in 2020. \n

In my research, I am always thinking about how to design practical algorithms and systems for sequential decision making under uncertainty. \n
My recent research interests focus on the intersection between *agent alignment*, *foundation models* and *reinforcement learning*.
\n

== Experience

- Applied Scientist Intern, Amazon Web Search (*AWS*) AI Research and Education lab, Santa Clara, May 2023 - Sep 2023. \n
Work on finetuning large language model with reinforcement leraning. (Mentor: [https://kaixianglin.github.io/ Kaixiang Lin]) \n



== Representative / Recent Works (\* indicates equal contribution.)

- [https://arxiv.org/abs/2302.07457 Understanding Expertise through Demonstrations: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning] \n 
    *Siliang Zeng\**, Chenliang Li\*, Alfredo Garcia, Mingyi Hong \n
    Thirty-seventh Conference on Neural Information Processing Systems (*NeurIPS*) 2023. (*Oral*) \n


- [http://arxiv.org/abs/2210.01282 Structural Estimation of Markov Decision Processes in High-Dimensional
  State Space with Finite-Time Guarantees] \n 
    *Siliang Zeng*, Mingyi Hong, Alfredo Garcia \n
    Under major revision at *Operations Research*.

- [https://openreview.net/forum?id=zbt3VmTsRIj&referrer=%5Bthe%20profile%20of%20Siliang%20Zeng%5D(%2Fprofile%3Fid%3D~Siliang_Zeng1) Maximum-Likelihood Inverse Reinforcement Learning with Finite-Time Guarantees] \n 
    *Siliang Zeng*, Chenliang Li, Alfredo Garcia, Mingyi Hong \n
    Thirty-sixth Conference on Neural Information Processing Systems (*NeurIPS 2022*). \n
    (A previous version accepted by Decision Awareness in Reinforcement Learning Workshop at ICML 2022) 


== Recent News
- Sep 2023: One paper has been accepted to *NeurIPS 2023* as an *oral*: [https://arxiv.org/abs/2302.07457 \n Understanding Expertise through Demonstrations: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning]. \n

- August 2023: One paper has been accepted to *Conference on Robot Learning (CoRL) 2023*: [https://openreview.net/forum?id=W5SrUCN0yUa&referrer=%5Bthe%20profile%20of%20Siliang%20Zeng%5D(%2Fprofile%3Fid%3D~Siliang_Zeng1) \n A Bayesian Approach to Robust Inverse Reinforcement Learning ]. \n

- May 2023: I join *Amazon Web Search (AWS)* as an applied scientist intern. I will work on reinforcement learning and large language model training.

- March 2023: I gave an invited talk at Amazon AWS about our recent work on offline inverse reinforcement learning: \n 
    [https://arxiv.org/abs/2302.07457 Understanding Expertise through Demonstrations: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning] \n 

- Oct 2022: I am thrilled to receive the *NeurIPS 2022 Scholar Award*. See you in New Orleans!

- Sep 2022: Two papers have been accepted to *NeurIPS 2022*: \n
    [https://openreview.net/forum?id=zbt3VmTsRIj&referrer=%5Bthe%20profile%20of%20Siliang%20Zeng%5D(%2Fprofile%3Fid%3D~Siliang_Zeng1) Maximum-Likelihood Inverse Reinforcement Learning with Finite-Time Guarantees.] \n
    [https://openreview.net/attachment?id=0RMDK39mGg&name=supplementary_material A Stochastic Linearized Augmented Lagrangian Method for Decentralized Bilevel Optimization.] \n
    #\n
    #This paper develops an inverse reinforcement learning framework, which aims to recover the nonlinear reward function through maximizing the Likelihood function over the expert trajectories. 
    #A novel single-loop algorithm for IRL is proposed, where each policy improvement step is followed by a stochastic gradient step for likelihood maximization. We show that the proposed algorithm provably converges to a stationary solution with a finite-time guarantee. 
    #Finally, by using robotics control problems in Mujoco and their transfer settings, we show that the proposed algorithm achieves superior performance compared with other IRL and imitation learning benchmarks. \n
    #\n

- Jul 2022: I have been selected for a travel grant from the DARL Workshop at ICML 2022! 
    #\n

- Jun 2022: One paper has been accepted to *Decision Awareness in Reinforcement Learning Workshop at ICML 2022*: [https://openreview.net/forum?id=FfELl5h3Nec \n Maximum-Likelihood Inverse Reinforcement Learning with Finite-Time Guarantees]. \n
    #\n
    #This paper develops an inverse reinforcement learning framework, which aims to recover the nonlinear reward function through maximizing the Likelihood function over the expert trajectories. 
    #A novel single-loop algorithm for IRL is proposed, where each policy improvement step is followed by a stochastic gradient step for likelihood maximization. We show that the proposed algorithm provably converges to a stationary solution with a finite-time guarantee. 
    #Finally, by using robotics control problems in Mujoco and their transfer settings, we show that the proposed algorithm achieves superior performance compared with other IRL and imitation learning benchmarks. \n
    #\n

- Jun 2022: One paper has been accepted to *SIAM Journal on Optimization*: [https://arxiv.org/abs/2006.11662 \n On the Divergence of Decentralized Non-Convex Optimization]. \n
    #\n
    #We study a generic class of decentralized algorithms in which N agents jointly optimize sum local objectives. By constructing some counter-examples, we show that when certain local Lipschitz conditions (LLC) on the local function gradients are not satisfied, 
    #most of the existing decentralized algorithms diverge, even if the global Lipschitz condition (GLC) is satisfied, where the sum function f has Lipschitz gradient. We then design a first-order algorithm, which is capable of computing stationary solutions with neither the LLC nor the GLC. \n
    #\n

- Mar 2022: One paper has been accepted to *L4DC 2022*: [https://arxiv.org/abs/2110.05597 \n Learning to Coordinate in Multi-Agent Systems: A Coordinated Actor-Critic Algorithm and Finite-Time Guarantees]. \n
    #\n
    #This paper develops a novel collaboration mechanism for designing robust MARL systems with theoretical guarantees. Specifically, we propose a class of coordinated actor-critic (*CAC*) algorithms where agents partially share their individually parametrized policies with neighbors.
    #We  conduct extensive numerical experiments and present theoretical analysis to demonstrate the
    #effectiveness of the proposed algorithm.\n
    #\n

- Sep 2021: One paper has been accepted to *NeurIPS 2021*: [https://arxiv.org/abs/2102.07367 \n A Near-Optimal Algorithm for Stochastic Bilevel Optimization via Double-Momentum]. \n
    #\n
    #This paper proposes a new algorithm – the Single-timescale Double-momentum Stochastic Approximation (*SUSTAIN*) – for tackling stochastic unconstrained bilevel optimization problems. 
    #The proposed algorithm achieves the best sample complexity for certain class of bi-level optimization problem.

